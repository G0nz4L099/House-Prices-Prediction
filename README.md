# House-Prices-Prediction
En este proyecto se realizo EDA  - Limpieza de datos - Faceture - Feature Engineering - Entrenamiento y Evaluacion de modelos 

Adjunto Una presentacion realizada para mi Acadamia + Codigo



## üíº El Problema de Negocio
La tasaci√≥n inmobiliaria tradicional presenta desaf√≠os cr√≠ticos:
*   **Subjetividad:** Dependencia del criterio humano.
*   **Ineficiencia:** Procesos lentos y costosos.
*   **P√©rdida de Capital:** Una tasaci√≥n incorrecta genera p√©rdida de liquidez (precio muy alto) o venta por debajo del mercado (precio muy bajo).

**Objetivo:** Crear un algoritmo que procese 80+ variables estructurales y de ubicaci√≥n para generar una "fair valuation" (precio justo) instant√°nea.

---

## üíæ Dataset y Tecnolog√≠as
El proyecto utiliza el famoso dataset **Ames Housing** (Iowa), conocido por su complejidad dimensional.

*   **Registros:** 1460 Propiedades.
*   **Variables:** 81 (Mixtas: Num√©ricas, Categ√≥ricas Nominales y Ordinales).
*   **Stack Tecnol√≥gico:**
    *   `Python`: Lenguaje principal.
    *   `Pandas` & `Numpy`: Manipulaci√≥n de datos.
    *   `Seaborn` & `Matplotlib`: Visualizaci√≥n estad√≠stica.
    *   `Scikit-Learn`: Modelado, Preprocesamiento y PCA.
    *   `XGBoost`: Gradient Boosting.

---

## üõ† Metodolog√≠a y Feature Engineering
Para lograr una alta precisi√≥n, se aplic√≥ un flujo de trabajo riguroso:

### 1. Limpieza y Tratamiento de Datos
*   **Imputaci√≥n Inteligente:** Manejo de valores nulos (NaN) basado en l√≥gica de negocio (ej. NaN en Garage significa "Sin Garage", no un error).
*   **Outliers:** Eliminaci√≥n de propiedades con >4000 pies cuadrados (GrLivArea) que distorsionaban la regresi√≥n.

### 2. Feature Engineering (Ingenier√≠a de Caracter√≠sticas)
*   **Transformaci√≥n Logar√≠tmica (`np.log1p`):** Aplicada al *Target* (SalePrice) y variables sesgadas (Skewed features) para normalizar la distribuci√≥n y mejorar el rendimiento de modelos lineales.
*   **Clustering de Vecindarios (K-Means):** Se re-agruparon los barrios en 4 cl√∫steres basados en comportamiento econ√≥mico y caracter√≠sticas estructurales, mejorando la detecci√≥n de zonas "Premium".
*   **Creaci√≥n de Variables:**
    *   `Total_M2_cub`: Suma de s√≥tano + superficie habitable.
    *   `HouseAge`: Antig√ºedad real al momento de venta.
    *   `OverallScore`: Puntuaci√≥n ponderada de calidad y condici√≥n.

### 3. Dimensionalidad (PCA)
Se experiment√≥ con **An√°lisis de Componentes Principales (PCA)** para eliminar la multicolinealidad. Aunque resolvi√≥ la redundancia de datos, se opt√≥ por el modelo con variables originales debido a una mayor interpretabilidad y precisi√≥n final.

---

## üìä Modelado y Resultados

Se entrenaron y optimic√© (usando **GridSearchCV** y **RandomizedSearchCV**) los siguientes modelos:
1.  **Lasso Regression (Regularizaci√≥n L1)**
2.  **Ridge Regression (Regularizaci√≥n L2)**
3.  **Random Forest Regressor**
4.  **XGBoost Regressor**
5.  **Huber Regressor (Robusto a outliers)**

### üèÜ El Modelo Ganador: Lasso (Optimizado)
A pesar de la popularidad de los modelos de ensamblaje (XGBoost), **Lasso** fue el ganador indiscutible.

| Modelo | RMSE (D√≥lares) | R¬≤ (Test) | Conclusi√≥n |
| :--- | :--- | :--- | :--- |
| **Lasso (Optimizado)** | **$18,852** | **0.907** | **Mejor generalizaci√≥n y precisi√≥n.** |
| Ridge | $19,844 | 0.902 | Muy cercano, pero menor selecci√≥n de variables. |
| Lasso (con PCA) | $21,375 | 0.889 | Estable, pero p√©rdida de informaci√≥n clave. |
| XGBoost | $21,280 | 0.914 | Tendencia al *Overfitting* en este dataset. |



## üë®‚Äçüíª Autor
**Gonzalo Cueto** - *Data Scientist & Python Developer*

*   [LinkedIn](www.linkedin.com/in/gonzalo-cueto-escalante)
*   

> **Insight T√©cnico:** La transformaci√≥n logar√≠tmica de las variables "linealiz√≥" el problema lo suficiente como para que un modelo lineal penalizado (Lasso) superara a los modelos de √°rboles, ofreciendo una soluci√≥n m√°s simple, interpretable y efectiva.
